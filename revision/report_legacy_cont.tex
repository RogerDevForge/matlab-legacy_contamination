\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\graphicspath{{D:/5th Semester/Legacy_contamination/Exercises (English)/Figures/}}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{float}
\usepackage{cite}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{parskip}
\usepackage{tikz}

\title{Exercise: Identification of Pollution Concentrations in a Shallow Groundwater Aquifer using Data-Driven Approaches}
\author{Roger Roca Miró \\ Legacy Contamination and Soil Remediation \\ TU Bergakademie Freiberg}
\date{\today}

\usepackage{titling}
\setlength{\droptitle}{-60pt}

\begin{document}
	
	\maketitle
	
	\section{Introduction}
	Groundwater contamination is a critical issue with significant environmental and public health implications. Accurate prediction of pollutant concentrations is essential for effective remediation and risk management. In this report, we apply the methodology of Taherdangkoo et al. \cite{taherdangkoo2020} by adapting advanced neural network training algorithms—Levenberg–Marquardt (LM) and Bayesian Regularization (BR).

	The Levenberg–Marquardt algorithm, introduced by Hagan and Menhaj (1994), is recognized for its computational efficiency and rapid convergence when training feedforward neural networks. MATLAB implements this algorithm in the \texttt{trainlm} function, which has become one of the most widely used tools for training neural networks due to its reliability and speed \cite{hagan1994,matlab}.

	The Bayesian Regularization algorithm, detailed by Foresee and Hagan (1997) \cite{foresee1997,matlab}, enhances model generalization through a Bayesian probabilistic framework. Its implementation in MATLAB as the \texttt{trainbr} function integrates the training and validation phases, providing an optimal solution for limited datasets common in groundwater contamination scenarios. This method reduces the risk of overfitting by automatically adjusting regularization parameters during training, effectively improving predictive accuracy and robustness.

	Our approach simulates contaminant migration in a shallow groundwater aquifer and compares the performance of both algorithms while evaluating the influence of key hydrogeological parameters using MATLAB.
	
	\section{Methodology}
	\subsection{Conceptual Model and Boundary Conditions}
	The 2D generic contamination model domain spans 150 m in length and 100 m in width, and is set to be located at a depth of 30 m (depth being just relevant to make clear that it represents a shallow aquifer). Two leakage sources are positioned along the left boundary, while an observation well is placed on the right side of the domain. The key input parameters for the simplified model are permeability, \textit{porosity}, and \textit{layer length}, while the output is the contaminant concentration.
	
	Groundwater flows from left to right under single-phase flow conditions, with the contaminant treated as a conservative tracer (i.e., sorption and degradation processes are neglected). Pressure head boundaries are applied laterally, and no-flow conditions are enforced at the top and bottom boundaries. All strata are assumed to be homogeneous and isotropic.
	The contaminant is allowed to leak into the aquifer over a simulation period of 150 hours. \textit{Figure \ref{fig:contamination_model}} presents a detailed diagram of the proposed model.
	
	\begin{figure}[h]
		\centering
		\begin{tikzpicture}
			
			% Draw the rectangular domain
			\fill[gray!50] (0,0) rectangle (15,10);
			
			% Draw the vertical partitions
			\draw (5,0) -- (5,10);
			\draw (10,0) -- (10,10);
			
			% Draw sources and observation well
			\fill (2.5,3) circle (0.15);  % Source 1
			\fill (2.5,7) circle (0.15);  % Source 2
			\fill (12.5,5) circle (0.15); % Observation well
			
			% Add labels ABOVE the dots with some vertical separation
			\node[above=5pt, red] at (2.5,3) {\textbf{Source 1}};
			\node[above=5pt, red] at (2.5,7) {\textbf{Source 2}};
			\node[above=5pt, green] at (12.5,5) {\textbf{Observation well}};
			
			% Axes and dimensions
			\draw[thick, ->] (-0.5,0) -- (-0.5,10);
			\draw[thick, ->] (0,-0.5) -- (15,-0.5);
			
			% Rotated label for width
			\node[rotate=90] at (-1.2,5) {\textbf{Width = 100 m}};
			
			% Label for length
			\node at (7.5,-1) {\textbf{Length = 150 m}};
			
		\end{tikzpicture}
		\caption{\textit{2D generic contamination model to investigate flow and transport of contaminant plumes in a shallow aquifer. Modified in \LaTeX{} from the WS 2023 Prof. Taherdangkoo slides.}}
		\label{fig:contamination_model}
	\end{figure}
	
	\subsection{Neural Network Structures and Training Algorithms}
	
	A feedforward neural network with one hidden layer is employed to predict pollutant concentrations in shallow aquifers. Similar neural network structures have been applied in recent groundwater contamination studies by Sandra M. Guzman et. al. (2017) \cite{guzman2017}. and Taherdangkoo et. al. (2020) \cite{taherdangkoo2020}.
	
	\begin{itemize}
		\item \textit{Data Normalization:} Input and target data are normalized to the range  using the \textit{mapminmax} function, since training algorithms typically perform better with data centered at zero \cite{guzman2017,taherdangkoo2020}.
		
		\item \textit{Network Architecture:} The hidden layer uses a hyperbolic tangent sigmoid (\textit{tansig}) \cite{matlab-tansig} transfer function, chosen over the default \textit{logsig} \cite{matlab-logsig} due to the normalized data range of \([-1, 1]\). The output layer applies a linear transfer function to facilitate continuous-value prediction.
		
		\item \textit{Training Algorithms:} As explained earlier, the two algorithms used are the LM and the BR. LM algorithm is favored in the field, for fast convergence and efficiency, particularly in scenarios requiring rapid training and reliable predictions \cite{taherdangkoo2020}.
		
		And regarding BR, which often integrates training and validation sets, enhancing generalization \cite{guzman2017,taherdangkoo2020,foresee1997}, we still separated the validation set. The reason was to ensure that both algorithms were evaluated under the same conditions. The validation partition is effectively inert for stopping in the BR case, but it still allows us to compute an even validation error metric for both algorithms.
		
		\item \textit{Performance Evaluation:} Model performance is assessed via performance metrics. The MSE, RMSE and $R^{2}$ are computed separately on the training, validation and test sets, per every number of neurons defining every model. The model yielding the lowest validation MSE is identified as the best model for each algorithm.
		
		\item \textit{Parameter Importance Analysis:} To quantify how input variables (\textit{permeability}, \textit{porosity}, and \textit{layer length}) influence prediction accuracy, the method Leave-One-Variable-Out (LOVO) is employed in a simplified way, from Olden et. al. (2004) \cite{olden2004}. The degradation in network performance upon omission of each input parameter is quantified, yielding a relative importance metric.
		
	\end{itemize}
	
	\section{Results and Discussion}
	Both Levenberg-Marquardt (LM) and Bayesian Regularization (BR) algorithms demonstrated strong predictive capabilities, with the LM network achieving its best performance using 8 neurons (Training MSE: 3.293448e+04, Validation MSE: 4.731353e+04, Testing MSE: 7.251373e+04) and the BR network performing optimally with 27 neurons (Training MSE: 1.355082e+04, Testing MSE: 3.161464e+04), turning up to be the superior algorithm.
	\textit{Figure~\ref{fig:comparison}} presents the performance comparison between the LM and BR algorithms in terms of MSE versus the number of neurons. The LM network shows separate curves for training, validation, and testing, with its best performance selected based on the validation error. In contrast, the BR network demonstrates a more robust performance without the advantage of avoiding the validation split.
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.05\textwidth]{algorithm_comparison.png}
		\caption{\textit{Algorithm Performance Comparison: MSE vs. number of neurons for LM (training, validation, and testing) and BR (training and testing). Circular markers indicate the best performing networks for each algorithm.}}
		\label{fig:comparison}
	\end{figure}
	The regression analyses for the LM and BR networks are illustrated in \textit{Figure~\ref{fig:lm_reg}} and \textit{Figure~\ref{fig:br_reg}}, respectively. For the LM network (\textit{Figure~\ref{fig:lm_reg}}), scatter plots of true versus predicted values for training, validation, and testing datasets indicate a strong correlation along the 45-degree line. Similarly, the BR network (\textit{Figure~\ref{fig:br_reg}}) shows a better agreement between predicted and true pollutant concentrations, confirming the better suitability for this application.
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{lm_regression.png}
		\caption{\textit{LM Network Regression Analysis: Scatter plots for (a) training, (b) validation, and (c) testing datasets with corresponding MSE values.}}
		\label{fig:lm_reg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{br_regression.png}
		\caption{\textit{BR Network Regression Analysis: Scatter plots for (a) training (combined with validation) and (b) testing datasets with corresponding MSE values.}}
		\label{fig:br_reg}
	\end{figure}
	For the parameter importance analysis the bar chart in \textit{Figure~\ref{fig:param_imp}} reveals the greater contribution of \textit{porosity} (0.510), over \textit{permeability} (0.207), and \textit{layer length} (0.283).
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.6\textwidth]{parameter_importance_analysis.png}
		\caption{\textit{Parameter Importance Analysis: Normalized importance of input parameters obtained by evaluating the impact of each parameter's exclusion on network performance.}}
		\label{fig:param_imp}
	\end{figure}
	Overall, the results demonstrate that the BR algorithm is more effective in predicting pollutant concentrations, and confirm \textit{porosity} as the most impactful parameter.
	
	\section{Conclusion}
	This study has demonstrated the application of data-driven neural network approaches for the identification of pollution concentrations in a shallow groundwater aquifer. A 2D generic contamination model was used to simulate contaminant transport under realistic boundary conditions. Neural networks with varying numbers of hidden neurons were trained using both Levenberg-Marquardt and Bayesian Regularization algorithms. 
	
	Both LM and BR networks provided accurate predictions, with the results making clear that the BR is the superior algorithm for the application. The LM network’s performance was optimized by monitoring validation MSE, while the BR network benefited from an automatic regularization mechanism. Parameter importance analysis highlighted that the \textit{porosity} is the main driver of pollution spread, while \textit{permeability} and \textit{layer length} can't be ignored.
	
	% References
	\bibliographystyle{apalike}
	
	\begin{thebibliography}{99} % 99 is the widest label (adjust as needed)
		\bibitem{foresee1997}
		Foresee, F. D., \& Hagan, M. T. (1997). Gauss–Newton approximation to Bayesian learning. In Proceedings of International Conference on Neural Networks (ICNN’97) (Vol. 3, pp. 1930–1935). IEEE. https://doi.org/10.1109/ICNN.1997.614194
		\bibitem{guzman2017}
		Guzman, Sandra M.; Paz, Joel O.; Tagert, Mary Love M. (2017): The Use of NARX Neural Networks to Forecast Daily Groundwater Levels. In Water Resour Manage 31 (5), pp. 1591–1603. DOI: 10.1007/s11269-017-1598-5.
		\bibitem{hagan1994}
		Hagan, M. T., \& Menhaj, M. B. (1994). Training feedforward networks with the Marquardt algorithm. IEEE Transactions on Neural Networks, 5(6), 989–993. https://doi.org/10.1109/72.329697
		\bibitem{matlab}
		MathWorks. (n.d.). Choose a multilayer neural network training function. MATLAB Documentation. Retrieved March 7, 2025, from https://www.mathworks.com/help/deeplearning/ug/choose-a-multilayer-neural-network-training-function.html
		\bibitem{matlab-tansig}
		MathWorks (n.d.). tansig: Hyperbolic tangent sigmoid transfer function. Retrieved from https://de.mathworks.com/help/deeplearning/ref/tansig.html.
		\bibitem{matlab-logsig}
		MathWorks (n.d.). logsig: Log-sigmoid transfer function. Retrieved from https://de.mathworks.com/help/deeplearning/ref/logsig.html.
		\bibitem{olden2004}
		Olden, J. D., Joy, M. K., \& Death, R. G. (2004). An accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data. Ecological Modelling, 178(3–4), 389–397.
		\bibitem{taherdangkoo2020}
		Taherdangkoo, R., Tatomir, A., Taherdangkoo, M., Qiu, P., \& Sauter, M. (2020). Nonlinear autoregressive neural networks to predict hydraulic fracturing fluid leakage into shallow groundwater. Water, 12(3), 841. https://doi.org/10.3390/w12030841
	\end{thebibliography}
	
\end{document}
